# Frontend Database Migration - Seamless Implementation Plan

**Date**: November 19, 2025  
**Target**: Zero-downtime deployment  
**Estimated Time**: 3-4 hours total (can be split across phases)  
**Risk Level**: LOW (all changes are additive, non-breaking)

---

## Executive Summary

This plan implements **6 database optimizations** to enhance frontend performance through a **phased, zero-downtime approach**. Each phase can be executed independently, allowing rollback at any point without data loss.

### Timeline Overview

```
Phase 1: Already Complete âœ…
  â†“
Phase 2: High Priority (NOW) - 1.5 hours
  â”œâ”€â”€ has_synthesis boolean
  â”œâ”€â”€ synthesis_preview column  
  â””â”€â”€ synthesis_word_count
  â†“
Phase 3: Medium Priority (Week 1) - 1 hour
  â”œâ”€â”€ JSON field counts
  â””â”€â”€ Processing metadata
  â†“
Phase 4: Low Priority (Month 1) - 30 min
  â”œâ”€â”€ Read time estimate
  â””â”€â”€ Verdict color hint
```

---

## Pre-Migration Checklist

### âœ… Prerequisites

- [x] Current database has `synthesis_article` column
- [x] Current database has `article_data` JSONB column
- [x] Test data exists (10 synthesis articles)
- [x] Alembic migrations configured
- [x] Database backup strategy in place

### ðŸ” Environment Check

```bash
# Verify current state
cd /Users/ej/Downloads/RSS-Feed/backend

# Check database connectivity
python3 -c "
from app.core.config import settings
print('Database URL:', settings.DATABASE_URL[:50] + '...')
print('Environment:', settings.ENVIRONMENT)
"

# Check current migration status
alembic current

# Verify test data
python3 -c "
import asyncio
from sqlalchemy import text
from app.core.config import settings
from sqlalchemy.ext.asyncio import create_async_engine

async def check():
    engine = create_async_engine(str(settings.DATABASE_URL), echo=False)
    async with engine.begin() as conn:
        result = await conn.execute(text('SELECT COUNT(*) FROM articles WHERE synthesis_article IS NOT NULL'))
        count = result.scalar()
        print(f'Articles with synthesis: {count}')
    await engine.dispose()

asyncio.run(check())
"
```

### ðŸ“Š Baseline Metrics

```sql
-- Capture baseline performance (run in psql or pgAdmin)
\timing on

-- Current table size
SELECT 
  pg_size_pretty(pg_total_relation_size('articles')) as total_size,
  pg_size_pretty(pg_relation_size('articles')) as table_size,
  pg_size_pretty(pg_indexes_size('articles')) as indexes_size;

-- Current query performance
EXPLAIN ANALYZE
SELECT id, title, fact_check_score, fact_check_verdict
FROM articles 
WHERE synthesis_article IS NOT NULL
ORDER BY published_date DESC
LIMIT 20;
```

**Record these values for comparison after migration**

---

## Phase 2: High Priority Optimizations

### Migration File Creation

Create: `alembic/versions/2025_11_19_2205-[hash]_add_frontend_helper_columns.py`

```python
"""add frontend helper columns

Revision ID: [auto-generated]
Revises: ba9c1d18afff
Create Date: 2025-11-19 22:05:00

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers
revision = '[auto-generated]'
down_revision = 'ba9c1d18afff'
branch_labels = None
depends_on = None


def upgrade() -> None:
    """Add frontend helper columns for synthesis mode display."""
    
    # Step 1: Add has_synthesis boolean (computed)
    op.execute("""
        ALTER TABLE articles 
        ADD COLUMN has_synthesis BOOLEAN 
          GENERATED ALWAYS AS (synthesis_article IS NOT NULL) STORED
    """)
    
    # Step 2: Add synthesis_word_count (computed)
    op.execute("""
        ALTER TABLE articles
        ADD COLUMN synthesis_word_count INTEGER 
          GENERATED ALWAYS AS (
            CASE 
              WHEN synthesis_article IS NOT NULL 
              THEN ARRAY_LENGTH(STRING_TO_ARRAY(synthesis_article, ' '), 1)
              ELSE NULL 
            END
          ) STORED
    """)
    
    # Step 3: Add synthesis_preview (computed, first 500 chars)
    op.execute("""
        ALTER TABLE articles
        ADD COLUMN synthesis_preview TEXT
          GENERATED ALWAYS AS (
            CASE 
              WHEN synthesis_article IS NOT NULL 
              THEN LEFT(synthesis_article, 500) 
              ELSE NULL 
            END
          ) STORED
    """)
    
    # Step 4: Add has_context_emphasis (computed)
    op.execute("""
        ALTER TABLE articles
        ADD COLUMN has_context_emphasis BOOLEAN
          GENERATED ALWAYS AS (article_data ? 'context_and_emphasis') STORED
    """)
    
    # Step 5: Add has_timeline (computed)
    op.execute("""
        ALTER TABLE articles
        ADD COLUMN has_timeline BOOLEAN
          GENERATED ALWAYS AS (article_data ? 'event_timeline') STORED
    """)
    
    # Step 6: Create indexes for efficient filtering
    op.create_index(
        'idx_articles_has_synthesis_v2',
        'articles',
        ['has_synthesis'],
        postgresql_where=sa.text('has_synthesis = true')
    )
    
    op.create_index(
        'idx_articles_synthesis_ready',
        'articles',
        ['has_synthesis', 'fact_check_score', 'fact_check_verdict'],
        postgresql_where=sa.text('has_synthesis = true')
    )
    
    # Step 7: Create full-text search index on preview
    op.execute("""
        CREATE INDEX idx_articles_synthesis_preview_fts
        ON articles USING GIN (to_tsvector('english', synthesis_preview))
        WHERE synthesis_preview IS NOT NULL
    """)
    
    print("âœ… Phase 2 migration complete: Frontend helper columns added")


def downgrade() -> None:
    """Remove frontend helper columns."""
    
    # Drop indexes first
    op.drop_index('idx_articles_synthesis_preview_fts', table_name='articles')
    op.drop_index('idx_articles_synthesis_ready', table_name='articles')
    op.drop_index('idx_articles_has_synthesis_v2', table_name='articles')
    
    # Drop columns (cascade will handle dependencies)
    op.execute("ALTER TABLE articles DROP COLUMN IF EXISTS has_timeline")
    op.execute("ALTER TABLE articles DROP COLUMN IF EXISTS has_context_emphasis")
    op.execute("ALTER TABLE articles DROP COLUMN IF EXISTS synthesis_preview")
    op.execute("ALTER TABLE articles DROP COLUMN IF EXISTS synthesis_word_count")
    op.execute("ALTER TABLE articles DROP COLUMN IF EXISTS has_synthesis")
    
    print("âœ… Phase 2 migration rolled back")
```

### Execution Steps

```bash
# 1. Create the migration file
cd /Users/ej/Downloads/RSS-Feed/backend
alembic revision --autogenerate -m "add frontend helper columns"

# 2. Edit the generated file to match the template above
# (Replace auto-generated content with the upgrade/downgrade functions)

# 3. Review the migration
alembic show head

# 4. Create database backup (IMPORTANT!)
# For Supabase: Use dashboard or CLI
# supabase db dump -f backup_before_phase2.sql

# 5. Apply migration
alembic upgrade head

# 6. Verify columns were added
python3 -c "
import asyncio
from sqlalchemy import text
from sqlalchemy.ext.asyncio import create_async_engine
from app.core.config import settings

async def verify():
    engine = create_async_engine(str(settings.DATABASE_URL), echo=False)
    async with engine.begin() as conn:
        result = await conn.execute(text('''
            SELECT 
                column_name, 
                data_type,
                is_generated
            FROM information_schema.columns
            WHERE table_name = \'articles\' 
              AND column_name IN (
                \'has_synthesis\', 
                \'synthesis_word_count\',
                \'synthesis_preview\',
                \'has_context_emphasis\',
                \'has_timeline\'
              )
            ORDER BY ordinal_position
        '''))
        
        print('\nPhase 2 Columns Added:')
        for row in result:
            print(f'  âœ… {row[0]:25} {row[1]:15} Generated: {row[2]}')
    
    await engine.dispose()

asyncio.run(verify())
"

# 7. Test queries with new columns
python3 -c "
import asyncio
from sqlalchemy import text
from sqlalchemy.ext.asyncio import create_async_engine
from app.core.config import settings

async def test_queries():
    engine = create_async_engine(str(settings.DATABASE_URL), echo=False)
    async with engine.begin() as conn:
        # Test 1: Filter by has_synthesis
        result = await conn.execute(text('''
            SELECT COUNT(*) 
            FROM articles 
            WHERE has_synthesis = true
        '''))
        print(f'Articles with synthesis: {result.scalar()}')
        
        # Test 2: Get preview and word count
        result = await conn.execute(text('''
            SELECT 
                title,
                synthesis_word_count,
                LENGTH(synthesis_preview) as preview_length
            FROM articles
            WHERE has_synthesis = true
            LIMIT 1
        '''))
        row = result.fetchone()
        if row:
            print(f'\nSample article:')
            print(f'  Title: {row[0][:50]}...')
            print(f'  Word count: {row[1]}')
            print(f'  Preview length: {row[2]} chars')
    
    await engine.dispose()

asyncio.run(test_queries())
"
```

### Post-Migration Validation

```bash
# 1. Check all synthesis articles have computed values
python3 scripts/testing/validate_phase2_migration.py
```

Create validation script:

```python
# scripts/testing/validate_phase2_migration.py
#!/usr/bin/env python3
import asyncio
from sqlalchemy import text
from sqlalchemy.ext.asyncio import create_async_engine
from app.core.config import settings

async def validate():
    engine = create_async_engine(str(settings.DATABASE_URL), echo=False)
    
    async with engine.begin() as conn:
        # Check all synthesis articles have computed values
        result = await conn.execute(text("""
            SELECT 
                COUNT(*) as total,
                COUNT(has_synthesis) as has_bool,
                COUNT(synthesis_word_count) as has_count,
                COUNT(synthesis_preview) as has_preview
            FROM articles
            WHERE synthesis_article IS NOT NULL
        """))
        
        row = result.fetchone()
        total, has_bool, has_count, has_preview = row
        
        print(f"\n{'='*60}")
        print("Phase 2 Migration Validation")
        print(f"{'='*60}\n")
        
        print(f"Total synthesis articles: {total}")
        print(f"With has_synthesis flag: {has_bool} ({'âœ… PASS' if has_bool == total else 'âŒ FAIL'})")
        print(f"With word count: {has_count} ({'âœ… PASS' if has_count == total else 'âŒ FAIL'})")
        print(f"With preview: {has_preview} ({'âœ… PASS' if has_preview == total else 'âŒ FAIL'})")
        
        # Check index usage
        result = await conn.execute(text("""
            SELECT indexname, indexdef 
            FROM pg_indexes 
            WHERE tablename = 'articles' 
              AND indexname LIKE '%synthesis%'
        """))
        
        print(f"\n{'='*60}")
        print("Indexes Created:")
        print(f"{'='*60}\n")
        
        for row in result:
            print(f"âœ… {row[0]}")
        
        print(f"\n{'='*60}")
        print("âœ… Phase 2 Migration Validated Successfully!")
        print(f"{'='*60}\n")
    
    await engine.dispose()

if __name__ == "__main__":
    asyncio.run(validate())
```

---

## Phase 3: Medium Priority Optimizations

### Migration File Creation

Create: `alembic/versions/2025_11_20_[hash]_add_json_field_counts.py`

```python
"""add json field counts and processing metadata

Revision ID: [auto-generated]
Revises: [phase2_revision]
Create Date: 2025-11-20

"""
from alembic import op
import sqlalchemy as sa

revision = '[auto-generated]'
down_revision = '[phase2_revision]'
branch_labels = None
depends_on = None


def upgrade() -> None:
    """Add JSON field counts and processing metadata."""
    
    # Step 1: Add JSON array length columns
    op.execute("""
        ALTER TABLE articles
        ADD COLUMN timeline_event_count INTEGER
          GENERATED ALWAYS AS (
            CASE 
              WHEN article_data ? 'event_timeline'
              THEN jsonb_array_length(article_data->'event_timeline')
              ELSE NULL
            END
          ) STORED
    """)
    
    op.execute("""
        ALTER TABLE articles
        ADD COLUMN reference_count INTEGER
          GENERATED ALWAYS AS (
            CASE 
              WHEN article_data ? 'references'
              THEN jsonb_array_length(article_data->'references')
              ELSE NULL
            END
          ) STORED
    """)
    
    op.execute("""
        ALTER TABLE articles
        ADD COLUMN margin_note_count INTEGER
          GENERATED ALWAYS AS (
            CASE 
              WHEN article_data ? 'margin_notes'
              THEN jsonb_array_length(article_data->'margin_notes')
              ELSE NULL
            END
          ) STORED
    """)
    
    # Step 2: Add processing metadata columns
    op.add_column('articles', 
        sa.Column('fact_check_mode', sa.String(20), nullable=True)
    )
    
    op.add_column('articles',
        sa.Column('fact_check_processing_time', sa.Integer, nullable=True,
                  comment='Processing time in seconds')
    )
    
    op.add_column('articles',
        sa.Column('synthesis_generated_at', sa.DateTime(timezone=True), nullable=True)
    )
    
    # Step 3: Create trigger for synthesis_generated_at
    op.execute("""
        CREATE OR REPLACE FUNCTION update_synthesis_timestamp()
        RETURNS TRIGGER AS $$
        BEGIN
          IF NEW.synthesis_article IS NOT NULL AND 
             (OLD.synthesis_article IS NULL OR OLD.synthesis_article = '') THEN
            NEW.synthesis_generated_at = NOW();
          END IF;
          RETURN NEW;
        END;
        $$ LANGUAGE plpgsql;
    """)
    
    op.execute("""
        CREATE TRIGGER trigger_synthesis_timestamp
          BEFORE UPDATE ON articles
          FOR EACH ROW
          EXECUTE FUNCTION update_synthesis_timestamp()
    """)
    
    # Step 4: Backfill synthesis_generated_at for existing rows
    op.execute("""
        UPDATE articles
        SET synthesis_generated_at = fact_checked_at
        WHERE synthesis_article IS NOT NULL 
          AND synthesis_generated_at IS NULL
    """)
    
    # Step 5: Create index for metrics queries
    op.create_index(
        'idx_articles_synthesis_metrics',
        'articles',
        ['timeline_event_count', 'reference_count'],
        postgresql_where=sa.text('has_synthesis = true')
    )
    
    print("âœ… Phase 3 migration complete: JSON field counts and metadata added")


def downgrade() -> None:
    """Remove JSON field counts and processing metadata."""
    
    # Drop trigger and function
    op.execute("DROP TRIGGER IF EXISTS trigger_synthesis_timestamp ON articles")
    op.execute("DROP FUNCTION IF EXISTS update_synthesis_timestamp()")
    
    # Drop index
    op.drop_index('idx_articles_synthesis_metrics', table_name='articles')
    
    # Drop columns
    op.drop_column('articles', 'synthesis_generated_at')
    op.drop_column('articles', 'fact_check_processing_time')
    op.drop_column('articles', 'fact_check_mode')
    op.execute("ALTER TABLE articles DROP COLUMN IF EXISTS margin_note_count")
    op.execute("ALTER TABLE articles DROP COLUMN IF EXISTS reference_count")
    op.execute("ALTER TABLE articles DROP COLUMN IF EXISTS timeline_event_count")
    
    print("âœ… Phase 3 migration rolled back")
```

### Execution Steps

```bash
# Same process as Phase 2
alembic revision -m "add json field counts and processing metadata"
# Edit file with template above
alembic upgrade head
# Run validation
```

---

## Phase 4: Low Priority Optimizations

### Migration File Creation

Create: `alembic/versions/2025_12_01_[hash]_add_ux_helper_columns.py`

```python
"""add ux helper columns (read time and verdict color)

Revision ID: [auto-generated]
Revises: [phase3_revision]
Create Date: 2025-12-01

"""
from alembic import op

revision = '[auto-generated]'
down_revision = '[phase3_revision]'
branch_labels = None
depends_on = None


def upgrade() -> None:
    """Add UX helper columns."""
    
    # Step 1: Add read time estimate
    op.execute("""
        ALTER TABLE articles
        ADD COLUMN synthesis_read_minutes INTEGER
          GENERATED ALWAYS AS (
            CASE 
              WHEN synthesis_word_count IS NOT NULL 
              THEN GREATEST(1, ROUND(synthesis_word_count::NUMERIC / 200.0))
              ELSE NULL
            END
          ) STORED
    """)
    
    # Step 2: Add verdict color hint
    op.execute("""
        ALTER TABLE articles
        ADD COLUMN verdict_color VARCHAR(20)
          GENERATED ALWAYS AS (
            CASE fact_check_verdict
              WHEN 'TRUE' THEN 'green'
              WHEN 'MOSTLY TRUE' THEN 'light-green'
              WHEN 'MOSTLY FALSE' THEN 'orange'
              WHEN 'FALSE' THEN 'red'
              WHEN 'UNVERIFIED - INSUFFICIENT EVIDENCE' THEN 'gray'
              ELSE 'neutral'
            END
          ) STORED
    """)
    
    print("âœ… Phase 4 migration complete: UX helper columns added")


def downgrade() -> None:
    """Remove UX helper columns."""
    
    op.execute("ALTER TABLE articles DROP COLUMN IF EXISTS verdict_color")
    op.execute("ALTER TABLE articles DROP COLUMN IF EXISTS synthesis_read_minutes")
    
    print("âœ… Phase 4 migration rolled back")
```

---

## Update Article Model

After each phase, update `app/models/article.py`:

```python
# app/models/article.py
from sqlalchemy import Column, Integer, String, Text, DateTime, Boolean, ARRAY, Index
from sqlalchemy.dialects.postgresql import UUID, JSONB
from sqlalchemy.orm import relationship
from app.core.database import Base
import uuid

class Article(Base):
    __tablename__ = "articles"

    # ... existing fields ...
    
    # Phase 1 (Already exists)
    synthesis_article = Column(Text, nullable=True, 
        comment="Full synthesis mode fact-check article (1,400-2,500 words)")
    
    # Phase 2 fields (computed)
    has_synthesis = Column(Boolean, 
        comment="Computed: True if synthesis_article exists")
    synthesis_word_count = Column(Integer,
        comment="Computed: Word count of synthesis article")
    synthesis_preview = Column(Text,
        comment="Computed: First 500 characters of synthesis article")
    has_context_emphasis = Column(Boolean,
        comment="Computed: True if context_and_emphasis in article_data")
    has_timeline = Column(Boolean,
        comment="Computed: True if event_timeline in article_data")
    
    # Phase 3 fields
    timeline_event_count = Column(Integer,
        comment="Computed: Number of timeline events in article_data")
    reference_count = Column(Integer,
        comment="Computed: Number of references in article_data")
    margin_note_count = Column(Integer,
        comment="Computed: Number of margin notes in article_data")
    fact_check_mode = Column(String(20), nullable=True,
        comment="Fact-check mode used: synthesis, iterative, thorough, standard")
    fact_check_processing_time = Column(Integer, nullable=True,
        comment="Processing time in seconds")
    synthesis_generated_at = Column(DateTime(timezone=True), nullable=True,
        comment="Timestamp when synthesis article was generated")
    
    # Phase 4 fields (computed)
    synthesis_read_minutes = Column(Integer,
        comment="Computed: Estimated read time in minutes (200 WPM)")
    verdict_color = Column(String(20),
        comment="Computed: Color hint for verdict display")
    
    # Indexes are defined in migration files
```

---

## Monitoring & Validation

### Performance Comparison

```sql
-- Before/After comparison
-- Run before Phase 2 and after Phase 4

-- Test 1: List query performance
EXPLAIN ANALYZE
SELECT id, title, fact_check_score, fact_check_verdict, synthesis_preview
FROM articles 
WHERE has_synthesis = true
ORDER BY published_date DESC
LIMIT 20;

-- Test 2: Stats query performance
EXPLAIN ANALYZE
SELECT 
  COUNT(*) as total,
  AVG(synthesis_word_count) as avg_words,
  AVG(timeline_event_count) as avg_events
FROM articles
WHERE has_synthesis = true;

-- Test 3: Storage impact
SELECT 
  pg_size_pretty(pg_total_relation_size('articles')) as total_size,
  pg_size_pretty(pg_relation_size('articles')) as table_size,
  pg_size_pretty(pg_indexes_size('articles')) as indexes_size;
```

### Health Checks

```bash
# Create monitoring script
# scripts/monitoring/check_synthesis_health.py
```

---

## Rollback Procedures

### Emergency Rollback (Any Phase)

```bash
# Rollback last migration
alembic downgrade -1

# Rollback to specific revision
alembic downgrade <revision_id>

# Rollback all Phase 4
alembic downgrade <phase3_revision>

# Complete rollback to Phase 1
alembic downgrade ba9c1d18afff
```

### Verify Rollback

```bash
python3 -c "
import asyncio
from sqlalchemy import text
from sqlalchemy.ext.asyncio import create_async_engine
from app.core.config import settings

async def check():
    engine = create_async_engine(str(settings.DATABASE_URL), echo=False)
    async with engine.begin() as conn:
        result = await conn.execute(text('''
            SELECT column_name 
            FROM information_schema.columns
            WHERE table_name = \'articles\' 
              AND column_name IN (
                \'has_synthesis\',
                \'timeline_event_count\',
                \'synthesis_read_minutes\'
              )
        '''))
        
        cols = [row[0] for row in result]
        print('Remaining Phase 2-4 columns:', cols if cols else 'None (clean rollback)')
    
    await engine.dispose()

asyncio.run(check())
"
```

---

## Timeline & Coordination

### Recommended Schedule

```
Day 1 (Today):
  - Review plan with team
  - Schedule maintenance window (optional, but recommended)
  - Backup database
  - Execute Phase 2 migration (1.5 hours)
  - Validate and monitor

Day 3-4:
  - Review Phase 2 metrics
  - Execute Phase 3 if beneficial (1 hour)
  - Update API endpoints to use new columns

Week 2-4:
  - Monitor usage patterns
  - Execute Phase 4 if needed (30 min)
  - Final optimization review
```

### Communication Plan

**Before Migration**:
- [ ] Notify frontend team of new columns
- [ ] Update API documentation
- [ ] Schedule deployment window

**During Migration**:
- [ ] Monitor database CPU/memory
- [ ] Watch for lock conflicts
- [ ] Track migration duration

**After Migration**:
- [ ] Verify all computed columns populated
- [ ] Update frontend code to use new fields
- [ ] Monitor query performance
- [ ] Document any issues

---

## Success Criteria

### Phase 2 Success
- âœ… All synthesis articles have `has_synthesis = true`
- âœ… Word counts match actual article word counts (Â±5%)
- âœ… Preview columns contain first 500 characters
- âœ… Indexes created successfully
- âœ… Query performance improved (list queries <10ms)

### Phase 3 Success
- âœ… JSON field counts accurate (match actual array lengths)
- âœ… Trigger fires on synthesis article updates
- âœ… Processing metadata queryable
- âœ… No performance degradation

### Phase 4 Success
- âœ… Read time estimates reasonable (word_count / 200)
- âœ… Verdict colors match expected mapping
- âœ… Frontend can use colors without mapping logic

---

## Final Checklist

### Pre-Deployment
- [ ] All migration files created
- [ ] Migration files tested locally
- [ ] Validation scripts written
- [ ] Rollback procedure documented
- [ ] Team notified

### Deployment
- [ ] Database backed up
- [ ] Phase 2 migration applied
- [ ] Validation tests passed
- [ ] Performance metrics captured
- [ ] Phase 3 migration applied (if proceeding)
- [ ] Phase 4 migration applied (if proceeding)

### Post-Deployment
- [ ] Frontend updated to use new columns
- [ ] API documentation updated
- [ ] Monitoring alerts configured
- [ ] Success metrics reported

---

**Status**: âœ… Plan Ready for Execution  
**Next Action**: Review with team, then execute Phase 2  
**Contact**: Backend team for questions  
**Last Updated**: November 19, 2025
